{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truths = []\n",
    "import os\n",
    "\n",
    "for file in os.listdir('/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/'):\n",
    "    latex_file_path = os.path.join(\"/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/\", file)\n",
    "    with open(latex_file_path, 'r', encoding='utf-8') as file:\n",
    "        predictions.append(file.read())\n",
    "        ground_truths.append(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd PDF_2_LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import edit_distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import orjson\n",
    "\n",
    "def compute_metrics(pred, gt, minlen=4):\n",
    "    metrics = {}\n",
    "    # if len(pred) < minlen or len(gt) < minlen:\n",
    "    #     return metrics\n",
    "    metrics[\"edit_dist\"] = edit_distance(pred, gt) / max(len(pred), len(gt))\n",
    "    reference = gt.split()\n",
    "    hypothesis = pred.split()\n",
    "    metrics[\"bleu\"] = nltk.translate.bleu([reference], hypothesis)\n",
    "    try:\n",
    "        metrics[\"meteor\"] = nltk.translate.meteor([reference], hypothesis)\n",
    "    except LookupError:\n",
    "        metrics[\"meteor\"] = np.nan\n",
    "    reference = set(reference)\n",
    "    hypothesis = set(hypothesis)\n",
    "    metrics[\"precision\"] = nltk.scores.precision(reference, hypothesis)\n",
    "    metrics[\"recall\"] = nltk.scores.recall(reference, hypothesis)\n",
    "    metrics[\"f_measure\"] = nltk.scores.f_measure(reference, hypothesis)\n",
    "    print(\"Inside compute_metrics\")\n",
    "    print(\"metrics\", metrics)\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "\n",
    "# for file in os.listdir('/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/'):\n",
    "#     latex_file_path = os.path.join('/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/', file)\n",
    "#     with open(latex_file_path, 'r', encoding='utf-8') as latex_file:\n",
    "#         latex_content = latex_file.read()\n",
    "#         # ground_truth = latex_file.read()\n",
    "#         predictions.append(latex_content)\n",
    "#         ground_truths.append(latex_content)\n",
    "        \n",
    "#         # Call compute_metrics directly\n",
    "#         metrics_for_file = compute_metrics(latex_content, latex_content)\n",
    "        \n",
    "#         for key, value in metrics_for_file.items():\n",
    "#             metrics[key].append(value)\n",
    "#             print(f\"{key}\", value)\n",
    "\n",
    "for file in os.listdir('/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/'):\n",
    "    latex_file_path = os.path.join(\"/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/\", file)\n",
    "    with open(latex_file_path, 'r', encoding='utf-8') as file:\n",
    "        outputs = file.read()\n",
    "        predictions.append(outputs)\n",
    "        ground_truths.append(outputs)\n",
    "        \n",
    "        with Pool(2) as p:\n",
    "            # print(\"inside POOL\")\n",
    "            _metrics = p.starmap(compute_metrics, iterable=zip(outputs, outputs))\n",
    "            for m in _metrics:\n",
    "                for key, value in m.items():\n",
    "                    metrics[key].append(value)\n",
    "            print({key: sum(values) / len(values) for key, values in metrics.items()})\n",
    "\n",
    "scores = {}\n",
    "for metric, vals in metrics.items():\n",
    "    scores[f\"{metric}_accuracies\"] = vals\n",
    "    scores[f\"{metric}_accuracy\"] = np.mean(vals)\n",
    "try:\n",
    "    print(\n",
    "        f\"Total number of samples: {len(vals)}, Edit Distance (ED) based accuracy score: {scores['edit_dist_accuracy']}, BLEU score: {scores['bleu_accuracy']}, METEOR score: {scores['meteor_accuracy']}\"\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "    scores[\"predictions\"] = predictions\n",
    "    scores[\"ground_truths\"] = ground_truths\n",
    "    with open('/home/patidarritesh/results.json', \"w\") as f:\n",
    "        json.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m30\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m     26\u001b[0m     files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap(process_file, files)\n\u001b[1;32m     29\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, vals \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "def process_file(file):\n",
    "    latex_file_path = os.path.join(\"/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/\", file)\n",
    "    with open(latex_file_path, 'r', encoding='utf-8') as file:\n",
    "        outputs = file.read()\n",
    "        predictions.append(outputs)\n",
    "        ground_truths.append(outputs)\n",
    "\n",
    "        # Call compute_metrics directly\n",
    "        metrics_for_file = compute_metrics(outputs, outputs)\n",
    "\n",
    "        for key, value in metrics_for_file.items():\n",
    "            metrics[key].append(value)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool(2) as p:\n",
    "        files = os.listdir('/home/patidarritesh/PDF_2_LaTeX/pdf_2_tex/dataset/root/latex/')\n",
    "        p.map(process_file, files)\n",
    "\n",
    "    scores = {}\n",
    "    for metric, vals in metrics.items():\n",
    "        scores[f\"{metric}_accuracies\"] = vals\n",
    "        scores[f\"{metric}_accuracy\"] = np.mean(vals)\n",
    "\n",
    "    try:\n",
    "        print(\n",
    "            f\"Total number of samples: {len(vals)}, Edit Distance (ED) based accuracy score: {scores['edit_dist_accuracy']}, BLEU score: {scores['bleu_accuracy']}, METEOR score: {scores['meteor_accuracy']}\"\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scores[\"predictions\"] = predictions\n",
    "    scores[\"ground_truths\"] = ground_truths\n",
    "    with open('/home/patidarritesh/results.json', \"w\") as f:\n",
    "        json.dump(scores, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
