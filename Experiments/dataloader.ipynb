{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.7 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.8 PyMuPDFb-1.23.7\n",
      "Requirement already satisfied: Pillow in /home/patidarritesh/miniconda3/envs/venv/lib/python3.11/site-packages (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting attrs>=19.2.0 (from jsonlines)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m439.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: attrs, jsonlines\n",
      "Successfully installed attrs-23.1.0 jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import random\n",
    "from typing import Dict, Tuple, Callable\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from typing import List, Optional\n",
    "import pypdf\n",
    "import orjson\n",
    "import jsonlines     \n",
    "import fitz  # PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00001.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00012.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00034.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00070.pdf\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "json_path = Path('/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/train.jsonl')\n",
    "\n",
    "# Open the JSONL file for reading\n",
    "with jsonlines.open(json_path) as reader:\n",
    "    for line_number, line in enumerate(reader, start=1):\n",
    "        # Process each line as needed\n",
    "        print(line['pdf'])\n",
    "\n",
    "print(line_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "# Implements image augmentation\n",
    "\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "\n",
    "def alb_wrapper(transform):\n",
    "    def f(im):\n",
    "        return transform(image=np.asarray(im))[\"image\"]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "class Erosion(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply erosion operation to an image.\n",
    "\n",
    "    Erosion is a morphological operation that shrinks the white regions in a binary image.\n",
    "\n",
    "    Args:\n",
    "        scale (int or tuple/list of int): The scale or range for the size of the erosion kernel.\n",
    "            If an integer is provided, a square kernel of that size will be used.\n",
    "            If a tuple or list is provided, it should contain two integers representing the minimum\n",
    "            and maximum sizes for the erosion kernel.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        if type(scale) is tuple or type(scale) is list:\n",
    "            assert len(scale) == 2\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = (scale, scale)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, tuple(np.random.randint(self.scale[0], self.scale[1], 2))\n",
    "        )\n",
    "        img = cv2.erode(img, kernel, iterations=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Dilation(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply dilation operation to an image.\n",
    "\n",
    "    Dilation is a morphological operation that expands the white regions in a binary image.\n",
    "\n",
    "    Args:\n",
    "        scale (int or tuple/list of int): The scale or range for the size of the dilation kernel.\n",
    "            If an integer is provided, a square kernel of that size will be used.\n",
    "            If a tuple or list is provided, it should contain two integers representing the minimum\n",
    "            and maximum sizes for the dilation kernel.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        if type(scale) is tuple or type(scale) is list:\n",
    "            assert len(scale) == 2\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = (scale, scale)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, tuple(np.random.randint(self.scale[0], self.scale[1], 2))\n",
    "        )\n",
    "        img = cv2.dilate(img, kernel, iterations=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Bitmap(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply a bitmap-style transformation to an image.\n",
    "\n",
    "    This transformation replaces all pixel values below a certain threshold with a specified value.\n",
    "\n",
    "    Args:\n",
    "        value (int, optional): The value to replace pixels below the threshold with. Default is 0.\n",
    "        lower (int, optional): The threshold value below which pixels will be replaced. Default is 200.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value=0, lower=200, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.lower = lower\n",
    "        self.value = value\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        img = img.copy()\n",
    "        img[img < self.lower] = self.value\n",
    "        return img\n",
    "\n",
    "\n",
    "train_transform = alb_wrapper(\n",
    "    alb.Compose(\n",
    "        [\n",
    "            Bitmap(p=0.05),\n",
    "            alb.OneOf([Erosion((2, 3)), Dilation((2, 3))], p=0.02),\n",
    "            alb.Affine(shear={\"x\": (0, 3), \"y\": (-3, 0)}, cval=(255, 255, 255), p=0.03),\n",
    "            alb.ShiftScaleRotate(\n",
    "                shift_limit_x=(0, 0.04),\n",
    "                shift_limit_y=(0, 0.03),\n",
    "                scale_limit=(-0.15, 0.03),\n",
    "                rotate_limit=2,\n",
    "                border_mode=0,\n",
    "                interpolation=2,\n",
    "                value=(255, 255, 255),\n",
    "                p=0.03,\n",
    "            ),\n",
    "            alb.GridDistortion(\n",
    "                distort_limit=0.05,\n",
    "                border_mode=0,\n",
    "                interpolation=2,\n",
    "                value=(255, 255, 255),\n",
    "                p=0.04,\n",
    "            ),\n",
    "            alb.Compose(\n",
    "                [\n",
    "                    alb.Affine(\n",
    "                        translate_px=(0, 5), always_apply=True, cval=(255, 255, 255)\n",
    "                    ),\n",
    "                    alb.ElasticTransform(\n",
    "                        p=1,\n",
    "                        alpha=50,\n",
    "                        sigma=120 * 0.1,\n",
    "                        alpha_affine=120 * 0.01,\n",
    "                        border_mode=0,\n",
    "                        value=(255, 255, 255),\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.04,\n",
    "            ),\n",
    "            alb.RandomBrightnessContrast(0.1, 0.1, True, p=0.03),\n",
    "            alb.ImageCompression(95, p=0.07),\n",
    "            alb.GaussNoise(20, p=0.08),\n",
    "            alb.GaussianBlur((3, 3), p=0.03),\n",
    "            alb.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "test_transform = alb_wrapper(\n",
    "    alb.Compose(\n",
    "        [\n",
    "            alb.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as alb\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# def alb_wrapper(transform):\n",
    "#     def f(im):\n",
    "#         return transform(image=np.asarray(im))[\"image\"]\n",
    "\n",
    "#     return f\n",
    "\n",
    "# train_transform = alb_wrapper(\n",
    "#         alb.Compose(\n",
    "#         [\n",
    "#             alb.Compose(\n",
    "#                 [alb.ShiftScaleRotate(shift_limit=0, scale_limit=(-.15, 0), rotate_limit=1, border_mode=0, interpolation=3,\n",
    "#                                     value=[255, 255, 255], p=1),\n",
    "#                 alb.GridDistortion(distort_limit=0.1, border_mode=0, interpolation=3, value=[255, 255, 255], p=.5)], p=.15),\n",
    "#             # alb.InvertImg(p=.15),\n",
    "#             alb.RGBShift(r_shift_limit=15, g_shift_limit=15,\n",
    "#                         b_shift_limit=15, p=0.3),\n",
    "#             alb.GaussNoise(10, p=.2),\n",
    "#             alb.RandomBrightnessContrast(.05, (-.2, 0), True, p=0.2),\n",
    "#             alb.ImageCompression(95, p=.3),\n",
    "#             alb.ToGray(always_apply=True),\n",
    "#             alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "#             # alb.Sharpen()\n",
    "#             ToTensorV2(),\n",
    "            \n",
    "#         ]\n",
    "#     )\n",
    "# )\n",
    "# test_transform = alb.Compose(\n",
    "#     [\n",
    "#         alb.ToGray(always_apply=True),\n",
    "#         alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "#         # alb.Sharpen()\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patidarritesh/miniconda3/envs/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from PIL import ImageOps\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "\n",
    "class enc():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = [896,16800]\n",
    "        self.align_long_axis = True\n",
    "        self.training = True\n",
    "\n",
    "    def crop_margin(self, img: Image.Image) -> Image.Image:\n",
    "        data = np.array(img.convert(\"L\"))\n",
    "        data = data.astype(np.uint8)\n",
    "        max_val = data.max()\n",
    "        min_val = data.min()\n",
    "        if max_val == min_val:\n",
    "            return img\n",
    "        data = (data - min_val) / (max_val - min_val) * 255\n",
    "        gray = 255 * (data < 200).astype(np.uint8)\n",
    "\n",
    "        coords = cv2.findNonZero(gray)  # Find all non-zero points (text)\n",
    "        a, b, w, h = cv2.boundingRect(coords)  # Find minimum spanning bounding box\n",
    "        return img.crop((a, b, w + a, h + b))\n",
    "\n",
    "    # def to_tensor(self, img: Image.Image):\n",
    "    #     if self.training:\n",
    "    #         return train_transform\n",
    "    #     else:\n",
    "    #         return test_transform\n",
    "    def to_tensor(self, img: Image.Image):\n",
    "        if self.training:\n",
    "            # return train_transform(image = img)['image']\n",
    "            return transforms.ToTensor()(img)\n",
    "        else:\n",
    "            return transforms.ToTensor()(img)\n",
    "    def prepare_input(self, pdf_path:str, random_padding: bool = False):\n",
    "        self.input_tensor = None\n",
    "        if pdf_path is None:\n",
    "            return\n",
    "        img_list = []\n",
    "        # Convert PDF to images using PyMuPDF\n",
    "        input_size = [896,672]\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_number in range(len(doc)):\n",
    "            page = doc[page_number]\n",
    "            image = page.get_pixmap()\n",
    "            # Convert Pixmap to PIL Image\n",
    "            img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "\n",
    "            # crop margins\n",
    "            try:\n",
    "                img = self.crop_margin(img.convert(\"RGB\"))\n",
    "            except OSError:\n",
    "                # might throw an error for broken files\n",
    "                return\n",
    "            if img.height == 0 or img.width == 0:\n",
    "                return\n",
    "            if self.align_long_axis and (\n",
    "                (input_size[0] > input_size[1] and img.width > img.height)\n",
    "                or (input_size[0] < input_size[1] and img.width < img.height)\n",
    "            ):\n",
    "                img = rotate(img, angle=-90, expand=True)\n",
    "            img = resize(img, min(input_size))\n",
    "            img.thumbnail((input_size[1], input_size[0]))\n",
    "            delta_width = input_size[1] - img.width\n",
    "            delta_height = input_size[0] - img.height\n",
    "            if random_padding:\n",
    "                pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "                pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "            else:\n",
    "                pad_width = delta_width // 2\n",
    "                pad_height = delta_height // 2\n",
    "            padding = (\n",
    "                pad_width,\n",
    "                pad_height,\n",
    "                delta_width - pad_width,\n",
    "                delta_height - pad_height,\n",
    "            )\n",
    "            padded_img = ImageOps.expand(img, padding)\n",
    "            img_list.append(padded_img)\n",
    "\n",
    "            # page_tensor = self.to_tensor(ImageOps.expand(img, padding))\n",
    "            # page_tensor = self.to_tensor(img)\n",
    "            # print(page_tensor)\n",
    "            # page_tensor=page_tensor.to('cuda')\n",
    "            # print(page_tensor.shape)\n",
    "            # if self.input_tensor is None:\n",
    "            #    self.input_tensor = page_tensor\n",
    "            # else:\n",
    "            #     self.input_tensor = torch.cat([self.input_tensor, page_tensor], dim=2)\n",
    "\n",
    "\n",
    "                # Calculate the padding to be added\n",
    "        target_shape = (3, 896, self.input_size[1])\n",
    "        original_shape = self.input_tensor.shape\n",
    "        padding = [0, target_shape[2] - original_shape[2]]\n",
    "\n",
    "        # Apply padding using torch.nn.functional.pad\n",
    "        padded_tensor = torch.nn.functional.pad(self.input_tensor, padding)\n",
    "\n",
    "        return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class pdfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, split: str = \"train\"):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.pdf_path = []\n",
    "        self.latex_path = []    \n",
    "\n",
    "        with jsonlines.open(self.dataset_path) as reader:\n",
    "            for line_number, line in enumerate(reader, start=1):\n",
    "                self.pdf_path.append(line['pdf'])\n",
    "                self.latex_path.append(line['latex'])\n",
    "        self.dataset_length = line_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pdf_path = self.pdf_path[idx]\n",
    "        latex_path = self.latex_path[idx]\n",
    "\n",
    "        # print(\"pdf: \", pdf_path)\n",
    "        # print(\"latex: \", latex_path)\n",
    "        encoder = enc()\n",
    "        input_tensor = encoder.prepare_input(pdf_path, random_padding=True)\n",
    "\n",
    "        print(\"input_tensor: \", input_tensor)\n",
    "        print(\"input_tensor.shape: \", input_tensor.shape)\n",
    "\n",
    "        # with open(latex_path, \"rb\") as f:\n",
    "        #     gnd_truth_data = f.read()\n",
    "        #     try:\n",
    "        #         gnd_truth_data = gnd_truth_data.decode(\"utf-8\")  # Try decoding with UTF-8\n",
    "        #     except:\n",
    "        #         gnd_truth_data = gnd_truth_data.decode(\"latin-1\", errors=\"ignore\")  # Fallback to Latin-1, ignore errors\n",
    "\n",
    "        # # print(\"gnd_truth_data: \", gnd_truth_data)\n",
    "        # tokenizer_out = self.nougat_model.decoder.tokenizer(\n",
    "        #     gnd_truth_data,\n",
    "        #     max_length=self.max_length,\n",
    "        #     padding=\"max_length\",\n",
    "        #     return_token_type_ids=False,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # input_ids = tokenizer_out[\"input_ids\"].squeeze(0)\n",
    "        # attention_mask = tokenizer_out[\"attention_mask\"].squeeze(0)\n",
    "        \"\"\"      \n",
    "        # randomly perturb ground truth tokens\n",
    "        if self.split == \"train\" and self.perturb:\n",
    "            # check if we perturb tokens\n",
    "            unpadded_length = attention_mask.sum()\n",
    "            while random.random() < 0.1:\n",
    "                try:\n",
    "                    pos = random.randint(1, unpadded_length - 2)\n",
    "                    token = random.randint(\n",
    "                        23, len(self.nougat_model.decoder.tokenizer) - 1\n",
    "                    )\n",
    "                    input_ids[pos] = token\n",
    "                except ValueError:\n",
    "                    break\"\"\"\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "\n",
    "        # Save input_tensor to JSON file\n",
    "        # json_path = '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'\n",
    "        # with open(json_path, \"w\") as file:\n",
    "        #     json.dump(input_tensor.tolist(), file)\n",
    "        \n",
    "\n",
    "\n",
    "        return pdf_path, latex_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pdfDataset(dataset_path=\"/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/train.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor:  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "input_tensor.shape:  torch.Size([3, 896, 16800])\n",
      "iteration:: 0 tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    print(\"iteration::\",i, sample)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding Details\n",
    "import json\n",
    "json_path = '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'\n",
    "with open(json_path, \"r\") as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(loaded_data[0])):\n",
    "    count =0\n",
    "    for i in range(len(loaded_data[0][0])):\n",
    "        if loaded_data[0][j][i]!=0 and loaded_data[0][0][i]!=1:\n",
    "            # print(loaded_data[0][0][i])\n",
    "            count+=1\n",
    "    print(\"count: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"class pdfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                dataset_path,\n",
    "                split: str = \"traisn\",\n",
    "                root_name: str = \"pdf\",):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.root_name = root_name\n",
    "        self.path_to_root = Path(dataset_path).parent\n",
    "        self.root_name = root_name\n",
    "        # self.path_to_root = self.path_to_root.parent\n",
    "\n",
    "        # if not split in self.dataset_path:\n",
    "        #     pti = self.path_to_root / (template % split + \".jsonl\")\n",
    "        #     if pti.exists():\n",
    "        #         self.dataset_path = pti\n",
    "        #     else:\n",
    "        #         raise ValueError(f'Dataset file for split \"{split}\" not found: {pti}')\n",
    "        \n",
    "        self.dataset_file = None  # mulitprocessing\n",
    "        \n",
    "        if self.dataset_file is None:\n",
    "            self.dataset_file = Path(self.dataset_path).open()\n",
    "        line = self.dataset_file.readline()\n",
    "        data: Dict = orjson.loads(line)\n",
    "\n",
    "        pdf_path: Path = self.path_to_root / self.root_name / data.pop(\"pdf\")\n",
    "        latex_path: Path = self.path_to_root / self.root_name / data.pop(\"latex\")\n",
    "\n",
    "        self.dataset = {\"pdf\": pdf_path, \"ground_truth\": latex_path}\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.dataset[idx]\n",
    "        # print(\"idx:\", idx)\n",
    "        # print(\"sample:\", sample)\n",
    "        return sample\n",
    "        # if sample is None:\n",
    "        #     # if sample is broken choose another randomly\n",
    "        #     return self[random.randint(0, self.dataset_length - 1)]\n",
    "        # if sample is None or sample[\"image\"] is None or prod(sample[\"image\"].size) == 0:\n",
    "        #     input_tensor = None\n",
    "        # else:\n",
    "        #     input_tensor = self.nougat_model.encoder.prepare_input(\n",
    "        #         sample[\"image\"], random_padding=self.split == \"train\"\n",
    "                \n",
    "        #     )\n",
    "\n",
    "        # tokenizer_out = self.nougat_model.decoder.tokenizer(\n",
    "        #     sample[\"ground_truth\"],\n",
    "        #     max_length=self.max_length,\n",
    "        #     padding=\"max_length\",\n",
    "        #     return_token_type_ids=False,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # input_ids = tokenizer_out[\"input_ids\"].squeeze(0)\n",
    "        # attention_mask = tokenizer_out[\"attention_mask\"].squeeze(0)\n",
    "        # # randomly perturb ground truth tokens\n",
    "        # if self.split == \"train\" and self.perturb:\n",
    "        #     # check if we perturb tokens\n",
    "        #     unpadded_length = attention_mask.sum()\n",
    "        #     while random.random() < 0.1:\n",
    "        #         try:\n",
    "        #             pos = random.randint(1, unpadded_length - 2)\n",
    "        #             token = random.randint(\n",
    "        #                 23, len(self.nougat_model.decoder.tokenizer) - 1\n",
    "        #             )\n",
    "        #             input_ids[pos] = token\n",
    "        #         except ValueError:\n",
    "        #             break\n",
    "        # return input_tensor, input_ids, attention_mask\n",
    "    def __iter__(self):\n",
    "        for i in range(self.dataset_length):\n",
    "            yield self[i]\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
