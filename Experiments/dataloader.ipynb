{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.7 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.8 PyMuPDFb-1.23.7\n",
      "Requirement already satisfied: Pillow in /home/patidarritesh/miniconda3/envs/venv/lib/python3.11/site-packages (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting attrs>=19.2.0 (from jsonlines)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m439.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: attrs, jsonlines\n",
      "Successfully installed attrs-23.1.0 jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import random\n",
    "from typing import Dict, Tuple, Callable\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from typing import List, Optional\n",
    "import pypdf\n",
    "import orjson\n",
    "import jsonlines     \n",
    "import fitz  # PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00001.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00012.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00034.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00070.pdf\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "json_path = Path('/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/train.jsonl')\n",
    "\n",
    "# Open the JSONL file for reading\n",
    "with jsonlines.open(json_path) as reader:\n",
    "    for line_number, line in enumerate(reader, start=1):\n",
    "        # Process each line as needed\n",
    "        print(line['pdf'])\n",
    "\n",
    "print(line_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "# Implements image augmentation\n",
    "\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "\n",
    "def alb_wrapper(transform):\n",
    "    def f(im):\n",
    "        return transform(image=np.asarray(im))[\"image\"]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "class Erosion(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply erosion operation to an image.\n",
    "\n",
    "    Erosion is a morphological operation that shrinks the white regions in a binary image.\n",
    "\n",
    "    Args:\n",
    "        scale (int or tuple/list of int): The scale or range for the size of the erosion kernel.\n",
    "            If an integer is provided, a square kernel of that size will be used.\n",
    "            If a tuple or list is provided, it should contain two integers representing the minimum\n",
    "            and maximum sizes for the erosion kernel.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        if type(scale) is tuple or type(scale) is list:\n",
    "            assert len(scale) == 2\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = (scale, scale)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, tuple(np.random.randint(self.scale[0], self.scale[1], 2))\n",
    "        )\n",
    "        img = cv2.erode(img, kernel, iterations=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Dilation(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply dilation operation to an image.\n",
    "\n",
    "    Dilation is a morphological operation that expands the white regions in a binary image.\n",
    "\n",
    "    Args:\n",
    "        scale (int or tuple/list of int): The scale or range for the size of the dilation kernel.\n",
    "            If an integer is provided, a square kernel of that size will be used.\n",
    "            If a tuple or list is provided, it should contain two integers representing the minimum\n",
    "            and maximum sizes for the dilation kernel.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        if type(scale) is tuple or type(scale) is list:\n",
    "            assert len(scale) == 2\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = (scale, scale)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, tuple(np.random.randint(self.scale[0], self.scale[1], 2))\n",
    "        )\n",
    "        img = cv2.dilate(img, kernel, iterations=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Bitmap(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply a bitmap-style transformation to an image.\n",
    "\n",
    "    This transformation replaces all pixel values below a certain threshold with a specified value.\n",
    "\n",
    "    Args:\n",
    "        value (int, optional): The value to replace pixels below the threshold with. Default is 0.\n",
    "        lower (int, optional): The threshold value below which pixels will be replaced. Default is 200.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value=0, lower=200, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.lower = lower\n",
    "        self.value = value\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        img = img.copy()\n",
    "        img[img < self.lower] = self.value\n",
    "        return img\n",
    "\n",
    "\n",
    "train_transform = alb_wrapper(\n",
    "    alb.Compose(\n",
    "        [\n",
    "            Bitmap(p=0.05),\n",
    "            alb.OneOf([Erosion((2, 3)), Dilation((2, 3))], p=0.02),\n",
    "            alb.Affine(shear={\"x\": (0, 3), \"y\": (-3, 0)}, cval=(255, 255, 255), p=0.03),\n",
    "            alb.ShiftScaleRotate(\n",
    "                shift_limit_x=(0, 0.04),\n",
    "                shift_limit_y=(0, 0.03),\n",
    "                scale_limit=(-0.15, 0.03),\n",
    "                rotate_limit=2,\n",
    "                border_mode=0,\n",
    "                interpolation=2,\n",
    "                value=(255, 255, 255),\n",
    "                p=0.03,\n",
    "            ),\n",
    "            alb.GridDistortion(\n",
    "                distort_limit=0.05,\n",
    "                border_mode=0,\n",
    "                interpolation=2,\n",
    "                value=(255, 255, 255),\n",
    "                p=0.04,\n",
    "            ),\n",
    "            alb.Compose(\n",
    "                [\n",
    "                    alb.Affine(\n",
    "                        translate_px=(0, 5), always_apply=True, cval=(255, 255, 255)\n",
    "                    ),\n",
    "                    alb.ElasticTransform(\n",
    "                        p=1,\n",
    "                        alpha=50,\n",
    "                        sigma=120 * 0.1,\n",
    "                        alpha_affine=120 * 0.01,\n",
    "                        border_mode=0,\n",
    "                        value=(255, 255, 255),\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.04,\n",
    "            ),\n",
    "            alb.RandomBrightnessContrast(0.1, 0.1, True, p=0.03),\n",
    "            alb.ImageCompression(95, p=0.07),\n",
    "            alb.GaussNoise(20, p=0.08),\n",
    "            alb.GaussianBlur((3, 3), p=0.03),\n",
    "            alb.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "test_transform = alb_wrapper(\n",
    "    alb.Compose(\n",
    "        [\n",
    "            alb.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def alb_wrapper(transform):\n",
    "    def f(im):\n",
    "        return transform(image=np.asarray(im))[\"image\"]\n",
    "\n",
    "    return f\n",
    "\n",
    "train_transform = alb_wrapper(\n",
    "        alb.Compose(\n",
    "        [\n",
    "            alb.Compose(\n",
    "                [alb.ShiftScaleRotate(shift_limit=0, scale_limit=(-.15, 0), rotate_limit=1, border_mode=0, interpolation=3,\n",
    "                                    value=[255, 255, 255], p=1),\n",
    "                alb.GridDistortion(distort_limit=0.1, border_mode=0, interpolation=3, value=[255, 255, 255], p=.5)], p=.15),\n",
    "            # alb.InvertImg(p=.15),\n",
    "            alb.RGBShift(r_shift_limit=15, g_shift_limit=15,\n",
    "                        b_shift_limit=15, p=0.3),\n",
    "            alb.GaussNoise(10, p=.2),\n",
    "            alb.RandomBrightnessContrast(.05, (-.2, 0), True, p=0.2),\n",
    "            alb.ImageCompression(95, p=.3),\n",
    "            alb.ToGray(always_apply=True),\n",
    "            alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "            # alb.Sharpen()\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "test_transform = alb.Compose(\n",
    "    [\n",
    "        alb.ToGray(always_apply=True),\n",
    "        alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "        # alb.Sharpen()\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from PIL import ImageOps\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "\n",
    "class enc():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = [896,672]\n",
    "        self.align_long_axis = True\n",
    "        self.training = True\n",
    "\n",
    "    def crop_margin(self, img: Image.Image) -> Image.Image:\n",
    "        data = np.array(img.convert(\"L\"))\n",
    "        data = data.astype(np.uint8)\n",
    "        max_val = data.max()\n",
    "        min_val = data.min()\n",
    "        if max_val == min_val:\n",
    "            return img\n",
    "        data = (data - min_val) / (max_val - min_val) * 255\n",
    "        gray = 255 * (data < 200).astype(np.uint8)\n",
    "\n",
    "        coords = cv2.findNonZero(gray)  # Find all non-zero points (text)\n",
    "        a, b, w, h = cv2.boundingRect(coords)  # Find minimum spanning bounding box\n",
    "        return img.crop((a, b, w + a, h + b))\n",
    "\n",
    "    # def to_tensor(self, img: Image.Image):\n",
    "    #     if self.training:\n",
    "    #         return train_transform\n",
    "    #     else:\n",
    "    #         return test_transform\n",
    "    def to_tensor(self, img: Image.Image):\n",
    "        if self.training:\n",
    "            # return train_transform(image = img)['image']\n",
    "            return transforms.ToTensor()(img)\n",
    "        else:\n",
    "            return transforms.ToTensor()(img)\n",
    "    def prepare_input(self, pdf_path:str, random_padding: bool = False):\n",
    "        self.input_tensor = None\n",
    "        if pdf_path is None:\n",
    "            return\n",
    "        \n",
    "        # Convert PDF to images using PyMuPDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_number in range(len(doc)):\n",
    "            page = doc[page_number]\n",
    "            image = page.get_pixmap()\n",
    "            # Convert Pixmap to PIL Image\n",
    "            img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "\n",
    "            # crop margins\n",
    "            try:\n",
    "                img = self.crop_margin(img.convert(\"RGB\"))\n",
    "            except OSError:\n",
    "                # might throw an error for broken files\n",
    "                return\n",
    "            if img.height == 0 or img.width == 0:\n",
    "                return\n",
    "            if self.align_long_axis and (\n",
    "                (self.input_size[0] > self.input_size[1] and img.width > img.height)\n",
    "                or (self.input_size[0] < self.input_size[1] and img.width < img.height)\n",
    "            ):\n",
    "                img = rotate(img, angle=-90, expand=True)\n",
    "            img = resize(img, min(self.input_size))\n",
    "            img.thumbnail((self.input_size[1], self.input_size[0]))\n",
    "            delta_width = self.input_size[1] - img.width\n",
    "            delta_height = self.input_size[0] - img.height\n",
    "            if random_padding:\n",
    "                pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "                pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "            else:\n",
    "                pad_width = delta_width // 2\n",
    "                pad_height = delta_height // 2\n",
    "            padding = (\n",
    "                pad_width,\n",
    "                pad_height,\n",
    "                delta_width - pad_width,\n",
    "                delta_height - pad_height,\n",
    "            )\n",
    "            \n",
    "            page_tensor = self.to_tensor(ImageOps.expand(img, padding))\n",
    "            # page_tensor = self.to_tensor(img)\n",
    "            # print(page_tensor)\n",
    "            # page_tensor=page_tensor.to('cuda')\n",
    "            # print(page_tensor.shape)\n",
    "            if self.input_tensor is None:\n",
    "               self.input_tensor = page_tensor\n",
    "            else:\n",
    "                self.input_tensor = torch.cat([self.input_tensor, page_tensor], dim=2)\n",
    "        return self.input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class pdfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, split: str = \"train\"):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.pdf_path = []\n",
    "        self.latex_path = []    \n",
    "\n",
    "        with jsonlines.open(self.dataset_path) as reader:\n",
    "            for line_number, line in enumerate(reader, start=1):\n",
    "                self.pdf_path.append(line['pdf'])\n",
    "                self.latex_path.append(line['latex'])\n",
    "        self.dataset_length = line_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pdf_path = self.pdf_path[idx]\n",
    "        latex_path = self.latex_path[idx]\n",
    "\n",
    "        # print(\"pdf: \", pdf_path)\n",
    "        # print(\"latex: \", latex_path)\n",
    "        encoder = enc()\n",
    "        input_tensor = encoder.prepare_input(pdf_path, random_padding=True)\n",
    "\n",
    "        print(\"input_tensor: \", input_tensor)\n",
    "        print(\"input_tensor.shape: \", input_tensor.shape)\n",
    "\n",
    "        with open(latex_path, \"rb\") as f:\n",
    "            gnd_truth_data = f.read()\n",
    "            try:\n",
    "                gnd_truth_data = gnd_truth_data.decode(\"utf-8\")  # Try decoding with UTF-8\n",
    "            except:\n",
    "                gnd_truth_data = gnd_truth_data.decode(\"latin-1\", errors=\"ignore\")  # Fallback to Latin-1, ignore errors\n",
    "\n",
    "        # print(\"gnd_truth_data: \", gnd_truth_data)\n",
    "        tokenizer_out = self.nougat_model.decoder.tokenizer(\n",
    "            gnd_truth_data,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = tokenizer_out[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = tokenizer_out[\"attention_mask\"].squeeze(0)\n",
    "        \"\"\"      \n",
    "        # randomly perturb ground truth tokens\n",
    "        if self.split == \"train\" and self.perturb:\n",
    "            # check if we perturb tokens\n",
    "            unpadded_length = attention_mask.sum()\n",
    "            while random.random() < 0.1:\n",
    "                try:\n",
    "                    pos = random.randint(1, unpadded_length - 2)\n",
    "                    token = random.randint(\n",
    "                        23, len(self.nougat_model.decoder.tokenizer) - 1\n",
    "                    )\n",
    "                    input_ids[pos] = token\n",
    "                except ValueError:\n",
    "                    break\"\"\"\n",
    "        return input_tensor, input_ids, attention_mask\n",
    "\n",
    "\n",
    "\n",
    "        # Save input_tensor to JSON file\n",
    "        # json_path = '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'\n",
    "        # with open(json_path, \"w\") as file:\n",
    "        #     json.dump(input_tensor.tolist(), file)\n",
    "        \n",
    "\n",
    "\n",
    "        return pdf_path, latex_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pdfDataset(dataset_path=\"/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor:  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "input_tensor.shape:  torch.Size([3, 896, 5376])\n",
      "data:  \\documentclass{article}\n",
      "\\PassOptionsToPackage{numbers}{natbib}\n",
      "\\newtheorem{theorem}{Theorem}[section]\n",
      "\\newtheorem{lemma}[theorem]{Lemma}\n",
      "     \n",
      "\\newcommand{\\danielle}[1]{{\\color{orange}[Danielle: #1]}}\n",
      "\\newcommand{\\bernie}[1]{{\\color{red!50}[Bernie: #1]}}\n",
      "\\newcommand{\\nadim}[1]{{\\color{blue}[Nadim: #1]}}\n",
      "\\usetikzlibrary{arrows}\n",
      "\\renewcommand{\\bibname}{References}\n",
      "\\newcommand\\blfootnote[1]{\n",
      "  \\begingroup\n",
      "  \\renewcommand\\thefootnote{}\\footnote{#1}\n",
      "  \\addtocounter{footnote}{-1}\n",
      "  \\endgroup\n",
      "}\n",
      "\\title{Modeling Advection on Directed Graphs using  Mat\\'{e}rn Gaussian Processes for Traffic Flow}\n",
      "\\author{\n",
      "  Danielle C. Maddix\n",
      "    \n",
      "   \n",
      "   \\\\\n",
      "  Amazon Research\\\\\n",
      "  2795 Augustine Dr. \\\\\n",
      "  Santa Clara, CA 95054 \\\\\n",
      "  \\texttt{dmmaddix@amazon.com} \\\\\n",
      "  \n",
      "   \\And\n",
      "   Nadim Saad \\thanks{Work conducted during an internship with Amazon AI.} \\\\\n",
      "   Stanford University \\\\\n",
      "   450 Serra Mall \\\\\n",
      "   Stanford, CA 94305 \\\\\n",
      "   \\texttt{nsaad31@stanford.edu} \\\\\n",
      "   \\And \n",
      "   Yuyang Wang \\\\\n",
      "   Amazon Research \\\\\n",
      "   2795 Augustine Dr. \\\\\n",
      "   Santa Clara, CA 95054 \\\\\n",
      "   \\texttt{yuyawang@amazon.com} \\\\\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "}\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\\vspace{-0.5cm}\n",
      "\\begin{abstract}\n",
      "The transport of traffic flow can be modeled by the advection equation.\n",
      "Finite difference and finite volumes methods have been used to numerically solve this hyperbolic equation on a mesh.  \n",
      "Advection has also been modeled discretely on directed graphs using the graph advection operator \\cite{chapman2011, thesis}.  In this paper, we first show that we can reformulate this graph advection operator as a finite difference scheme. We then propose the Directed Graph Advection Mat\\'{e}rn Gaussian Process (DGAMGP) model that incorporates the dynamics of this graph advection operator into the kernel of a trainable Mat\\'{e}rn Gaussian Process to effectively model traffic flow and its uncertainty as an advective process on a directed graph.\n",
      "\\end{abstract}\n",
      "\\vspace{-.65cm}\n",
      "\\section{Introduction}\n",
      "\\vspace{-.25cm}\n",
      "The continuous linear advection equation models the flow of a scalar concentration along a vector field. \n",
      "  The solutions to this hyperbolic partial differential equation may develop discontinuities or shocks over time depending on the initial condition. These shocks can model the formation of traffic jams, and their propagation along a road \\citep{Richards1956}. \n",
      "Figure \\ref{fig:upwind_exact} illustrates an example, where initially the first half of the road is 70\\% occupied with cars, and the second half of the road is empty. The traffic propagates to the right until the whole road is 70\\% occupied. \n",
      "Classical methods, such as finite differences and finite volumes, have been used to predict the flow of traffic along a road \\cite{Lighthill1955OnKW, Richards1956}. \n",
      "These classical numerical methods do not incorporate any randomness into the model, and can be limited in incorporating the uncertainty among different driver's behaviors \\cite{driving}.\n",
      "\\vspace{-.25cm}\n",
      " \\begin{minipage}{\\linewidth}\n",
      "      \\centering\n",
      "      \\begin{minipage}{0.49\\linewidth} \n",
      "          \\begin{figure}[H]\n",
      "          \\centering\n",
      "              [width=\\linewidth]{sol_times_all.png}\n",
      "              \\caption{Propagation of cars on a road using an advection process.\\\\}\n",
      "              \\label{fig:upwind_exact}\n",
      "          \\end{figure}\n",
      "      \\end{minipage}\n",
      "    \n",
      "      \\begin{minipage}{0.5\\linewidth} \n",
      "      Gaussian processes (GPs) \\cite{rasmussen2006} can learn unknown functions that allow use of prior information about their properties and for uncertainty modeling. \\citet{kuper2020} propose the Gaussian Process Kalman Filter (GPKF) method to simulate spatiotemporal models, and test on the advection equation.  \\citet{RAISSI2019686} train GPs on data to learn the underlying physics of non-linear advection-diffusion equations. Additional physics-based machine learning models \\cite{borovitskiy2021matern} use the Matérn covariance function given below:\n",
      "      \\begin{equation} u \\sim N\\big(0,\\big(\\frac{2\\nu}{\\kappa^2} + \\Delta \\big)^{-\\nu} \\big), \n",
      "\\label{eqn:Matern_GP}\n",
      "\\end{equation}\n",
      "      \\end{minipage}\n",
      "  \\end{minipage}\n",
      "   where $u$ denotes an unknown function, $\\nu < \\infty$, $\\kappa < \\infty$ and $\\Delta$ denotes the laplacian \\cite{bakka2020diffusionbased}.  \n",
      " The Matérn kernel captures physical processes due to its finite differentiability, and is also commonly used to define distances between two points that are $d$ units distant from each other \\cite{borovitskiy2021matern}.\n",
      " \\citet{osti_1642956} propose training joint Mat\\'{e}rn GPs to model space-fractional differential equations, in which the advection-diffusion equation is a special case.\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "Recent works including \\cite{solomon2015pde} have studied solving partial differential equation (PDEs) on graphs. \\citet{chapman2011, thesis} propose discrete advection and consensus operators to model advection and diffusion flows, respectively on directed graphs.  \\citet{hosek2019} study the advection-diffusion equation on graphs using this discrete advection operator, and show that finite volume numerical discretizations can be reformulated as equations on graphs resulting in a corresponding maximum principle for this operator. Additional works have also looked at combining scientific computing and machine learning on graphs for spatiotemporal traffic modeling \\cite{li2018}.  \\citet{chamberlain2021grand} propose the Graph Neural Diffusion (GRAND) method, which combines traditional ODE solvers with graph neural networks (GNNs) to model diffusion on a undirected graph.  \\citet{borovitskiy2021matern} propose to replace the continuous laplacian $\\Delta$ in \\eqref{eqn:Matern_GP} with the discrete graph laplacian operator $L$ to model diffusion on undirected graphs, which can be limited for traffic modeling. \n",
      "  \n",
      "      \n",
      "    \n",
      "    \n",
      "                \n",
      "               \n",
      "                \n",
      "          \n",
      "The goal of this paper is two-fold: to develop a model that effectively models traffic flow as an advective process on a directed graph and its uncertainty.  We propose a novel method, Directed Graph Advection Mat\\'{e}rn Gaussian Process (DGAMGP) that uses a symmetric positive definite variant of the graph advection operator $L_{adv}$ as a covariance matrix in the Mat\\'{e}rn Gaussian Process.  We use the square of the singular values of $L_{adv}$ to model the advection dynamics, and train a Mat\\'{e}rn Gaussian Process to model the uncertainty.  We also show the connection between consistent finite difference stencils for solving the linear advection equation and the graph advection operator. Our novel linkage helps improve the understanding and interpretability of this graph advection operator. \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "                \n",
      "        \n",
      "                \n",
      "        \n",
      "                \n",
      "        \n",
      "                \n",
      "        \n",
      "        \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "      \n",
      " \n",
      " \n",
      " \n",
      "\\section{Understanding the directed graph advection operator}\n",
      "\\label{sect:l_adv_prop}\n",
      "We aim to model the continuous advection equation for unknown scalar $u$ under vector field $v$:\n",
      "\\begin{equation*}\n",
      "        \\frac{\\partial u}{\\partial t} = -\\nabla \\cdot (vu),\n",
      "        \\label{eqn:gov_eqn}\n",
      "    \\end{equation*}\n",
      "stochastically on a directed graph. \n",
      "We define a directed, weighted graph $\\mathcal{G} = (V, E, W)$ with $|V| = n$ nodes and $|E| = |W| = m$ edges, where $V$ denotes the vertex, $E$ the edge, and $W$ the edge weight \n",
      "sets, respectively. \n",
      "We discretize the flow $vu$ along edge $(i, j) \\in E$ with weight $w_{ji} \\in W$ as $w_{ji}u_i(t)$, where $u_i(t)$ denotes the concentration $u$ at node $i$ and time $t$. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "The graph advection operator $L_{adv}$ is defined so that the flow into a node equals the flow out of it \\cite{chapman2011}:\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "      \\begin{equation}\n",
      "     \\begin{aligned}\n",
      "       \\frac{du_i(t)}{dt} &= \\sum_{j: (j, i) \\in E} w_{ij}u_j(t) - \\sum_{j: (i, j) \\in E} w_{ji} u_i(t) = -[L_{adv}u(t)]_i,\\\\\n",
      "     \n",
      "       \n",
      "    \\end{aligned}       \n",
      "    \\label{eqn:graph_adv}\n",
      "    \\end{equation}\n",
      "where $L_{adv} = D_{out} - A_{in}$ \n",
      "for diagonal out-degree matrix $D_{out}$ and \n",
      "in-degree adjacency matrix $A_{in}$.  \n",
      "  For general directed graphs, $L_{adv}$ belongs to the square, non-symmetric with non-negative real part eigenvalues \\cite{thesis} class of matrices in \\cite{liesen2008}. \n",
      "   By design, $L_{adv}$ is conservative, unlike the related diffusion or consensus operator $L_{cons} = D_{in}-A_{in}$, where $D_{in}$ denotes the diagonal in-degree matrix \\cite{chapman2011, thesis}.  \n",
      " \n",
      "  \n",
      "  \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      " \n",
      "A main motivating reason for using $L_{adv}$ to model traffic flow is that it results in a conservative scheme. \n",
      "  \n",
      "   \n",
      "  \n",
      "\\vspace{-.25cm}\n",
      "\\paragraph{Reformulation of $L_{adv}$ as finite difference on balanced graphs.}\n",
      "\\label{subsec:finite_diff}\n",
      "We notice that $L_{adv}$ at node $i$ is a weighted linear combination of the other nodes adjacent to it, which resembles finite difference stencils of the unknown and its neighbors.  We make this connection precise, and then construct example graphs where $L_{adv}$ corresponds to common finite difference schemes for linear advection.\n",
      "\\begin{theorem}\n",
      "$L_{adv}$ corresponds to a semi-discrete finite difference advection scheme, where the sum of the coefficients is zero if and only if the graph $\\mathcal{G}$ is balanced, i.e. $L_{adv} = L_{cons}$.\n",
      "\\end{theorem}\n",
      "\\vspace{-.5cm}\n",
      "\\begin{proof}\n",
      "A finite difference approximation to the gradient can be written as the following weighted linear combination of its neighbors $u_j$ for arbitrary coefficients $c_{ij} \\in \\mathbb{R}$:\n",
      "\\begin{equation}\n",
      "    -(u_x)_i \\approx \\sum_{j \\ne i}c_{ij}u_j + c_{ii}u_i.\n",
      "\\label{eqn:FD}\n",
      "\\end{equation}\n",
      "A consistent finite difference scheme is at least zero-th order accurate \\cite{leveque}.   \n",
      "Since the derivative of a constant is 0, the coefficients must sum to 0, i.e $c_{ii} = -\\sum_{j \\ne i}c_{ij}$. \n",
      " \n",
      "Combining \n",
      "\\eqref{eqn:graph_adv} with \\eqref{eqn:FD} gives:\n",
      "$$(D_{out})_{ii} = \\sum_{j: (i, j) \\in E} w_{ji} = -c_{ii}  = \\sum_{j \\ne i}c_{ij} = \\sum_{j: (j, i) \\in E} w_{ij} = (D_{in})_{ii}.$$\n",
      "The graph $\\mathcal{G}$ is balanced by definition, and it follows that $L_{adv} = L_{cons}$. The other direction follows similarly.\n",
      "\\end{proof}\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Applying $L_{adv}$ on the directed line graph in Figure \\ref{fig:graph_upwind} results in the first order upwind scheme with spatial step size $\\Delta x$ for $v > 0$ in \\eqref{eqn:first_order_upwind} (See Appendix \\ref{appendix:upwind} and Figure \\ref{solve_upwind_conv} for the convergence study).\n",
      "Similarly, Figure \\ref{fig:graph_central} illustrates the directed graph in which $L_{adv}$ gives the second order central difference scheme, where $(u_x)_i \\approx(u_{i+1}-u_{i-1})/(2 \\Delta x)$ (See Appendix \\ref{app:finite_diff_graphs} for additional examples).\n",
      "\\begin{figure}[H]\n",
      "\\centering\n",
      "\\subfigure[first order upwind scheme]{\\label{fig:graph_upwind}\n",
      "\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,\n",
      "                    thick,main node/.style={circle,draw,font=\\sffamily\\bfseries}]\n",
      "                    \\centering\n",
      "  \\node[main node] (1) {$u_{i-1}$};\n",
      "  \\node[main node] (2) [right of=1] {$u_{i}$};\n",
      "  \\node[main node] (3) [right of=2] {$u_{i+1}$};\n",
      "  \\path[every node/.style={font=\\sffamily\\small}]\n",
      "    (1) edge node[above] {$v/\\Delta{x}$} (2)\n",
      "    (2) edge node [above] {$v/\\Delta{x}$} (3);\n",
      "    \\path (0,-1cm);\n",
      "\\end{tikzpicture}}\n",
      "\\subfigure[second order central scheme]{\\label{fig:graph_central}\n",
      "\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,\n",
      "                    thick,main node/.style={circle,draw,font=\\sffamily\\bfseries}]\n",
      "\\hspace{0.5cm}\n",
      " \n",
      "  \\node[main node] (1) {$u_{i-1}$};\n",
      "  \\node[main node] (2) [right of=1] {$u_{i}$};\n",
      "  \\node[main node] (3) [right of=2] {$u_{i+1}$};\n",
      "  \\path[every node/.style={font=\\sffamily\\small}]\n",
      "    (1) edge node[above] {$v/2\\Delta{x}$} (2)\n",
      "    (3) edge node [above] {$-v/2\\Delta{x}$} (2) \n",
      "    (2) edge [bend left] node[below] {$-v/2\\Delta{x}$} (1)\n",
      "    (2) edge [bend right] node[below] {$v/2\\Delta{x}$} (3);\n",
      "\\end{tikzpicture}}\n",
      "\\caption{Balanced graphs on which $L_{adv}$ corresponds to finite difference stencils of linear advection.}\n",
      "\\end{figure}\n",
      " \n",
      "\\section{Directed Graph Advection Mat\\'{e}rn Gaussian Process (DGAMGP)}\n",
      "\\label{sec:dgamgp}\n",
      "We propose the novel Directed Graph Advection Mat\\'{e}rn Gaussian Process (DGAMGP) model, which uses the dynamics of $L_{adv}$ to model advection stochastically on a directed graph through a discrete approximation to the continuous Laplacian $\\Delta$ of the Mat\\'{e}rn Gaussian Process in \\eqref{eqn:Matern_GP}. The covariance matrix or kernel $\\mathcal{K}$ of a Gaussian process needs to be symmetric and positive semi-definite.  This leads to some challenges with the $L_{adv}$ operator as it is not guaranteed in general to be symmetric or positive semi-definite (See Section \\ref{sect:l_adv_prop}).  Note that using the graph Laplacian $L$ in the covariance matrix in the undirected graph case is more straightforward since $L$ is symmetric positive semi-definite. \n",
      "In our directed graph case,  we propose using $L^T_{adv}L_{adv}$ as the covariance matrix since it is symmetric positive definite, and hence orthogonally diagonalizable.  Analogous to \\cite{borovitskiy2021matern}, we  define a function $\\phi$ of a diagonalizable matrix through Taylor series expansion.   Then we can define its eigendecomposition as $L^T_{adv}L_{adv} = X_{adv}\\Lambda_{adv} X^T_{adv}$, so that $\\phi(L^T_{adv}L_{adv}) = X_{adv}\\phi(\\Lambda_{adv}) X^T_{adv}$, where $\\phi(\\Lambda_{adv})$ is computed by applying $\\phi$ to the diagonal elements of $\\Lambda_{adv}$.  \n",
      "We compute the eigendecomposition of $L^T_{adv}L_{adv}\n",
      "= V_{adv} \\Sigma_{adv}^2 V_{adv}^T $, using the singular value decomposition (SVD) of $L_{adv} = U_{adv} \\Sigma_{adv} V_{adv}^T$, \n",
      " \n",
      "where the eigenvalues and eigenvectors are the singular values squared and right singular vectors of $L_{adv}$, respectively. \n",
      "Hence, we model the advection dynamics using the square of the singular values of $L_{adv}$.\n",
      "Our approach can also be viewed as adding the square of the singular values of $L_{adv}$ to the diagonal for regularization.  \n",
      "Computing the thin-SVD is more computationally efficient and numerically stable, since we avoid explicitly forming the matrix-matrix product $L^T_{adv}L_{adv}$, which has double the condition number of $L_{adv}$, and the numerical issues with then computing its eigendecomposition.\n",
      "We chose $\\phi$ to be the Mat\\'{e}rn covariance function in \\eqref{eqn:Matern_GP}, and our DGAMGP model is given by:\n",
      " \\begin{equation}\n",
      " u \\sim N\\big(0,\\big( V_{adv} (\\frac{2 \\nu}{\\kappa^2}I + \\Sigma^2_{adv})^{-\\nu} V^T_{adv}\\big) \\big).\n",
      "  \\label{eqn:directed_graph_gp}\n",
      " \\end{equation}\n",
      "This \n",
      "advective Gaussian Process is then trained on data by minimizing the negative log-likelihood of the Gaussian Process to learn the kernel hyperparameters $\\nu$ and $\\kappa$, and predict $u$ \\cite{gardner2018}. For inference, we draw samples from the GP predictive posterior distribution with the learned hyperparameters \\cite{rasmussen2006}.  See Algorithm \\ref{alg:pseudocode} for details.\n",
      " \n",
      " \n",
      " \n",
      "    \n",
      "    \n",
      "    \n",
      "          \n",
      "               \n",
      "           \n",
      "            \n",
      "         \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "            \n",
      "     \n",
      "\\paragraph{Choice of $L^T_{adv}L_{adv}$.} There are alternate approaches to symmetrize $L_{adv}$. The first simple approach explored is to utilize $L_{sym} = (L_{adv}^T + L_{adv}) / 2$ . This operator is not positive semi definite except in the balanced graph case. The second approach is to use the symmetrizer method in \\cite{sen1988}, which generates a symmetric matrix $L'_{sym}$ with the same eigenvalues as $L_{adv}$ but is not always positive semi definite.\n",
      "\\begin{algorithm}[H]\n",
      "\\caption{The Directed Graph Advection Mat\\'{e}rn Gaussian Process (DGAMGP)}\\label{alg:pseudocode}\n",
      "\\begin{algorithmic}\n",
      "    \\item \\textbf{Given} a directed graph $\\mathcal{G} = (V,E, W)$ and training data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$.\n",
      "  \n",
      "    \\begin{enumerate}\n",
      "    \\item Compute  $L_{adv}(\\mathcal{G}) = D_{out} - A_{in}$.\n",
      "    \n",
      "    \\item Compute the SVD of $L_{adv} = U_{adv} \\Sigma_{adv} V_{adv}^T$.\n",
      "    \n",
      "    \\item Generate a DGAMGP model in \\eqref{eqn:directed_graph_gp}. \n",
      "    \\item Minimize the GP negative log marginal likelihood using $\\mathcal{D}$ to learn $\\nu, \\kappa$ and $\\sigma$ \\cite{gardner2018}.\n",
      "    \\item Given test data $\\{x_i^*\\}$, draw samples from the GP predictive posterior distribution \\cite{rasmussen2006}.\n",
      "\\end{enumerate}\n",
      "\\end{algorithmic}\n",
      "\\end{algorithm}\n",
      "\\section{Numerical Results}\n",
      "\\label{sec:num_res}\n",
      "\\vspace{-0.25cm}\n",
      " \n",
      "          In this section, we utilize our DGAMGP model for traffic modeling on synthetic and real-world directed traffic graphs.  \n",
      "         \n",
      "        The data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ denotes the traffic flow speed in miles per hour $y_i$ at location $x_i$. We test our model's predictive ability to predict the velocities of cars on a road at different positions.  We use hold-out cross validation to split the data points generated into training (70\\% of the data) and testing data (30\\% of the data).  We extend the code in \\cite{borovitskiy2021matern} to compute the singular value decomposition of $L_{adv}$ to train our DGAMGP model on a directed graph.  The code is available at\n",
      "\\url{https://github.com/advectionmatern/Modeling-Advection-on-Directed-Graphs-using-Mat-e-rn-Gaussian-Processes}, and the experiments are run on Amazon Sagemaker \\cite{sagemaker}.\n",
      "\\paragraph{Regression results on synthetic graphs.}\n",
      " We generate synthetic data that models traffic along a road, which has a relatively high density of cars in the first half and a low density of cars in the second half. We train and test our model on the upwind scheme in Figure \\ref{fig:graph_upwind}, central scheme in Figure \\ref{fig:graph_central}, an intersecting lane graph, where two lanes merge into one lane in Figure \\ref{fig:merging_lanes} and a loop graph representing the upwind scheme with periodic boundary conditions in Figure \\ref{fig:loop}.  Table \\ref{tab:synthetic_results} compares the results to the consensus baseline model of using the singular value decomposition of $L_{cons}$ in Eqn. \\eqref{eqn:directed_graph_gp}. \n",
      "         \\begin{table}[h!]\n",
      "         \\small\n",
      "\\centering\n",
      "\\begin{tabular}{ |c|c|c|c|c|c|c|c| }\n",
      "\\hline\n",
      "Model & Graph type & $n = 280$ & $n = 325$ & $n = 400$ & $\\nu$ & $\\kappa$ & $\\sigma$\\\\ \\hline\n",
      "Advection & \\multirow{2}{*}{Upwind} & 0.52 & 0.45 & \\textbf{0.0005} &0.65 & 8.09 &  7.75  \\\\\n",
      "Consensus  &  & \\textbf{0.51} & \\textbf{0.44} & \\textbf{0.0005} &0.65 & 8.29 &  7.77  \\\\  \n",
      "\\hline\n",
      "Advection & \\multirow{2}{*}{Central}                    & 1.31 & 0.85 & 8.41e-05 & 0.67 & 9.00 & 8.03 \\\\ \n",
      "                                \n",
      "                 Consensus                    &                     & \\textbf{0.97} & \\textbf{0.8} & \\textbf{8.02e-05} & 0.67 & 9.45 & 8.11 \\\\\n",
      "                 \\hline\n",
      "Advection & \\multirow{2}{*}{Intersection} & 0.96 & \\textbf{0.45} & \\textbf{0.0005} & 0.65 & 8.19 & 7.75  \\\\\n",
      "Consensus  &  & \\textbf{0.52} & 0.46 & \\textbf{0.0005} & 0.64 & 8.28 & 7.77  \\\\ \n",
      "\\hline\n",
      "Advection & \\multirow{2}{*}{Loop} & \\textbf{0.47} & \\textbf{0.41} & \\textbf{0.00045} & 0.65 & 8.49 & 7.76  \\\\\n",
      "Consensus  &  & \\textbf{0.47} & \\textbf{0.41} & \\textbf{0.00045} & 0.65 & 8.49 & 7.76   \\\\ \n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\vspace{.25cm}\n",
      " \\caption{Comparison of $l_2$ test error on synthetic directed graphs with $n$ nodes and the learned hyperparameters.}\n",
      " \n",
      "\\label{tab:synthetic_results}\n",
      "\\end{table}\n",
      "         \n",
      "   \n",
      "       \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "  \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "        \n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "        \n",
      "       \n",
      "     \n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      "        \n",
      "        \n",
      "\\vspace{-.25cm}\n",
      "\\begin{figure}[H]\n",
      "\\centering\n",
      "\\subfigure[intersection graph]{\\label{fig:merging_lanes}\n",
      "\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.95cm,\n",
      "                    thick,main node/.style={circle,draw,font=\\sffamily\\bfseries}]\n",
      " \n",
      "  \\node[main node] (1) {$u_{i-2}$};\n",
      "  \\node[main node] (2) [right of=1] {$u_{i-1}$};\n",
      "  \\node[main node] (3) [right of=2] {$u_i$};\n",
      "  \\node[main node] (4) [below of=1] {$u_{i-4}$};\n",
      "    \\node[main node] (5) [below of=2] {$u_{i-3}$};\n",
      "    \\node[main node] (6) [right of=3] {$u_{i+1}$};\n",
      "  \\path[every node/.style={font=\\sffamily\\small}]\n",
      "    (1) edge node[above] {$v/\\Delta{x}$} (2)\n",
      "    (2) edge node [above] {$v/\\Delta{x}$} (3) \n",
      "    (4) edge node[above] {$v/\\Delta{x}$} (5)\n",
      "    (5) edge [bend right] node[below] {\\hspace{.5cm} $v/\\Delta{x}$} (3)\n",
      "    (3) edge node[above] {$2v/\\Delta{x}$} (6);\n",
      "\\end{tikzpicture}}\n",
      "\\subfigure[loop graph]{\\label{fig:loop}\n",
      "\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,\n",
      "                    thick,main node/.style={circle,draw,font=\\sffamily\\bfseries}]\n",
      " \n",
      "  \\node[main node] (1) {$u_1$};\n",
      "  \\node[main node] (2) [right of=1] {$u_{i-1}$};\n",
      "  \\node[main node] (3) [right of=2] {$u_i$};\n",
      "    \\node[main node] (4) [right of=3] {$u_n$};\n",
      "  \\path[every node/.style={font=\\sffamily\\small}]\n",
      "    (1) edge node[above] {$v/\\Delta{x}$} (2)\n",
      "    (2) edge node[above] {$v/\\Delta{x}$} (3)\n",
      "    (3) edge node[above] {$v/\\Delta{x}$} (4)\n",
      "    (4) edge [bend left] node[below] {$v/\\Delta{x}$} (1);\n",
      "\\end{tikzpicture}}\n",
      "\\caption{Graphs representing two lanes merging into one (\\textit{left}) and a loop (\\textit{right}).}\n",
      "\\end{figure}\n",
      " \\paragraph{Regression results on a real-world traffic graph.}\n",
      "      We test on the real-world traffic\n",
      "        data from the California Performance Measurement System \\cite{chen2001} with the road network graph from the San Jose highways from Open Street Map \\cite{osm2017} at a fixed time. Since our method supports directed graphs, we do not need to convert the raw directed traffic data to an undirected graph as in \\cite{borovitskiy2021matern}.  We use the same experimental setup from \\cite{borovitskiy2021matern} to generate the train and test data.  \n",
      "        Figure \\ref{fig:results-real-data} shows the resulting predictive mean and standard deviation of the speed on the San Jose highways using the visualization tools from \\cite{borovitskiy2021matern}.  \n",
      "        We notice that the predictive standard deviation along the nodes is relatively small, and is larger on the points that are farther from the sensors.\n",
      "   \\begin{figure}[H]\n",
      "        \\centering\n",
      "      \n",
      "     [width=\\linewidth]{results_real_data.png}\n",
      "              \\caption{Traffic speed interpolation over a graph of San Jose highways using our DGAMGP method with $\\nu = 0.35, \\kappa = 1002.8$, $\\sigma = 1.14$ and plotting tools from \\cite{borovitskiy2021matern}.}\n",
      "              \\label{fig:results-real-data}\n",
      "      \\end{figure}\n",
      " \n",
      " \n",
      "    \n",
      "   \n",
      "\\section{Conclusions}\n",
      "In this paper, we propose a novel method DGAMGP to model an advective process on a directed graph and its uncertainties. We show connections between finite differences schemes used to solve the linear advection equation and the graph advection operator $L_{adv}$ employed in our model. \n",
      "We explore a regression problem on various graphs, and show that our proposed DGAMGP model performs similarly to other state-of-the-art models.\n",
      "Future work includes adding a time-varying component to our model, comparing our method to classical numerical methods for solving PDEs, and incorporating the behavior of the non-linear advection equation for traffic modeling.\n",
      "\\begin{thebibliography}{}\n",
      "\\bibitem[Bakka et~al., 2020]{bakka2020diffusionbased}\n",
      "Bakka, H., Krainski, E., Bolin, D., Rue, H., and Lindgren, F. (2020).\n",
      "\\newblock The diffusion-based extension of the mat\\'ern field to space-time.\n",
      "\\newblock {\\em arXiv:2006.04917}.\n",
      "\\bibitem[Borovitskiy et~al., 2021]{borovitskiy2021matern}\n",
      "Borovitskiy, V., Azangulov, I., Terenin, A., Mostowsky, P., Deisenroth, M., and\n",
      "  Durrande, N. (2021).\n",
      "\\newblock Mat{é}rn gaussian processes on graphs.\n",
      "\\newblock In Banerjee, A. and Fukumizu, K., editors, {\\em Proceedings of The\n",
      "  24th International Conference on Artificial Intelligence and Statistics},\n",
      "  volume 130 of {\\em Proceedings of Machine Learning Research}, pages\n",
      "  2593--2601. PMLR.\n",
      "\\bibitem[Chamberlain et~al., 2021]{chamberlain2021grand}\n",
      "Chamberlain, B., Rowbottom, J., Gorinova, M.~I., Bronstein, M., Webb, S., and\n",
      "  Rossi, E. (2021).\n",
      "\\newblock Grand: Graph neural diffusion.\n",
      "\\newblock In Meila, M. and Zhang, T., editors, {\\em Proceedings of the 38th\n",
      "  International Conference on Machine Learning}, volume 139 of {\\em Proceedings\n",
      "  of Machine Learning Research}, pages 1407--1418. PMLR.\n",
      "\\bibitem[Chapman and Mesbahi, 2011]{chapman2011}\n",
      "Chapman, A. and Mesbahi, M. (2011).\n",
      "\\newblock Advection on graphs.\n",
      "\\newblock {\\em IEEE Conference on Decision and Control and European Control\n",
      "  Confereence (CDC-ECC)}, 50:1461--1466.\n",
      "\\bibitem[Chen et~al., 2001]{chen2001}\n",
      "Chen, C., Petty, K., Skabardonis, A., Varaiya, P., and Jia, Z. (2001).\n",
      "\\newblock Freeway performance measurement system: mining loop detector data.\n",
      "\\newblock {\\em Transportation Research Record}, 1748(1):96--102.\n",
      "\\bibitem[Chen et~al., 2018]{driving}\n",
      "Chen, Y., Sohani, N., and Peng, H. (2018).\n",
      "\\newblock Modelling of uncertain reactive human driving behavior: a\n",
      "  classification approach.\n",
      "\\newblock In {\\em 2018 IEEE Conference on Decision and Control (CDC)}, pages\n",
      "  3615--3621.\n",
      "\\bibitem[Gardner et~al., 2018]{gardner2018}\n",
      "Gardner, J., Pleiss, G., Bindel, D., Weinberger, K., and Wilson, A. (2018).\n",
      "\\newblock G{P}ytorch: Blackbox matrix-matrix gaussian process inference with\n",
      "  gpu acceleration.\n",
      "\\newblock {\\em 32nd Conference on Neural Information Processing Systems (NIPS\n",
      "  2018) arXiv:1809.11165v2}.\n",
      "\\bibitem[Gulian et~al., 2019]{osti_1642956}\n",
      "Gulian, M., Raissi, M., Perdikaris, P., and Karniadakis, G. (2019).\n",
      "\\newblock Machine learning of space-fractional differential equations, {SIAM}\n",
      "  {J}ournal on {S}cientific {C}omputing, {V}ol. 41, {N}o. 4, {S}ociety for\n",
      "  {I}ndustrial and {A}pplied {M}athematics.\n",
      "\\newblock pages A2485--A2509.\n",
      "\\bibitem[Hošek and Volek, 2019]{hosek2019}\n",
      "Hošek, R. and Volek, J. (2019).\n",
      "\\newblock Discrete advection–diffusion equations on graphs: Maximum principle\n",
      "  and finite volumes.\n",
      "\\newblock {\\em Applied Mathematics and Computation}, 361(C):630--644.\n",
      "\\bibitem[K\\\"{u}per and Waldherr, 2020]{kuper2020}\n",
      "K\\\"{u}per, A. and Waldherr, S. (2020).\n",
      "\\newblock Numerical gaussian process kalman filtering.\n",
      "\\newblock {\\em 21st IFAC World Congress}.\n",
      "\\bibitem[LeVeque, 2007]{leveque}\n",
      "LeVeque, R.~J. (2007).\n",
      "\\newblock {\\em Finite Difference Methods for Ordinary and Partial Differential\n",
      "  Equations: Steady-State and Time-Dependent Problems}.\n",
      "\\newblock SIAM.\n",
      "\\bibitem[Li et~al., 2018]{li2018}\n",
      "Li, Y., Yu, R., Shahabi, C., and Liu, Y. (2018).\n",
      "\\newblock Diffusion convolutional recurrent neural network: Data-driven traffic\n",
      "  forecasting.\n",
      "\\newblock {\\em International Conference on Learning Representations (ICLR)}.\n",
      "\\bibitem[Liberty et~al., 2020]{sagemaker}\n",
      "Liberty, E., Karnin, Z., Xiang, B., Rouesnel, L., Coskun, B., Nallapati, R.,\n",
      "  Delgado, J., Sadoughi, A., Astashonok, Y., Das, P., Balioglu, C.,\n",
      "  Chakravarty, S., Jha, M., Gautier, P., Arpin, D., Januschowski, T., Flunkert,\n",
      "  V., Wang, Y., Gasthaus, J., Stella, L., Rangapuram, S., Salinas, D.,\n",
      "  Schelter, S., and Smola, A. (2020).\n",
      "\\newblock Elastic machine learning algorithms in amazon sagemaker.\n",
      "\\newblock In {\\em 2020 ACM SIGMOD International Conference on Management of\n",
      "  Data, SIGMOD ’20, New York, NY, USA. Association for Computing Machinery.},\n",
      "  pages 731--737.\n",
      "\\bibitem[Liesen and Parlett, 2008]{liesen2008}\n",
      "Liesen, J. and Parlett, B.~N. (2008).\n",
      "\\newblock On nonsymmetric saddle point matrices that allow conjugate gradient\n",
      "  iterations.\n",
      "\\newblock {\\em Numer. Math.}, 108:605--624.\n",
      "\\bibitem[Lighthill and Whitham, 1955]{Lighthill1955OnKW}\n",
      "Lighthill, M. and Whitham, G. (1955).\n",
      "\\newblock On kinematic waves ii. a theory of traffic flow on long crowded\n",
      "  roads.\n",
      "\\newblock {\\em Proceedings of the Royal Society of London. Series A.\n",
      "  Mathematical and Physical Sciences}, 229:317 -- 345.\n",
      "\\bibitem[OpenStreetMap, 2017]{osm2017}\n",
      "OpenStreetMap (2017).\n",
      "\\newblock \\url {https://www.openstreetmap.org}.\n",
      "\\bibitem[Raissi et~al., 2019]{RAISSI2019686}\n",
      "Raissi, M., Perdikaris, P., and Karniadakis, G. (2019).\n",
      "\\newblock Physics-informed neural networks: A deep learning framework for\n",
      "  solving forward and inverse problems involving nonlinear partial differential\n",
      "  equations.\n",
      "\\newblock {\\em Journal of Computational Physics}, 378:686--707.\n",
      "\\bibitem[Rak, 2017]{thesis}\n",
      "Rak, A. (2017).\n",
      "\\newblock Advection on graphs.\n",
      "\\newblock \\url {http://nrs.harvard.edu/urn-3:HUL.InstRepos:38779537}.\n",
      "\\bibitem[Rasmussen and Williams, 2006]{rasmussen2006}\n",
      "Rasmussen, C. and Williams, C. (2006).\n",
      "\\newblock {\\em Gaussian Processes for Machine Learning}.\n",
      "\\newblock MIT Press.\n",
      "\\bibitem[Richards, 1956]{Richards1956}\n",
      "Richards, P. (1956).\n",
      "\\newblock Shock waves on the highway.\n",
      "\\newblock {\\em Operation Res.}, pages 42 -- 51.\n",
      "\\bibitem[Sen and Venkaiah, 1988]{sen1988}\n",
      "Sen, S. and Venkaiah, V.~C. (1988).\n",
      "\\newblock On symmetrizing a matrix.\n",
      "\\newblock {\\em Indian J. pure appl. Math.}, 19(6):554--561.\n",
      "\\bibitem[Solomon, 2015]{solomon2015pde}\n",
      "Solomon, J. (2015).\n",
      "\\newblock {PDE} approaches to graph analysis.\n",
      "\\newblock {\\em ArXiv}, abs/1505.00185.\n",
      "\\end{thebibliography}\n",
      "\\begin{thebibliography}{}\n",
      "\\bibitem[Bakka et~al., 2020]{bakka2020diffusionbased}\n",
      "Bakka, H., Krainski, E., Bolin, D., Rue, H., and Lindgren, F. (2020).\n",
      "\\newblock The diffusion-based extension of the mat\\'ern field to space-time.\n",
      "\\newblock {\\em arXiv:2006.04917}.\n",
      "\\bibitem[Borovitskiy et~al., 2021]{borovitskiy2021matern}\n",
      "Borovitskiy, V., Azangulov, I., Terenin, A., Mostowsky, P., Deisenroth, M., and\n",
      "  Durrande, N. (2021).\n",
      "\\newblock Mat{é}rn gaussian processes on graphs.\n",
      "\\newblock In Banerjee, A. and Fukumizu, K., editors, {\\em Proceedings of The\n",
      "  24th International Conference on Artificial Intelligence and Statistics},\n",
      "  volume 130 of {\\em Proceedings of Machine Learning Research}, pages\n",
      "  2593--2601. PMLR.\n",
      "\\bibitem[Chamberlain et~al., 2021]{chamberlain2021grand}\n",
      "Chamberlain, B., Rowbottom, J., Gorinova, M.~I., Bronstein, M., Webb, S., and\n",
      "  Rossi, E. (2021).\n",
      "\\newblock Grand: Graph neural diffusion.\n",
      "\\newblock In Meila, M. and Zhang, T., editors, {\\em Proceedings of the 38th\n",
      "  International Conference on Machine Learning}, volume 139 of {\\em Proceedings\n",
      "  of Machine Learning Research}, pages 1407--1418. PMLR.\n",
      "\\bibitem[Chapman and Mesbahi, 2011]{chapman2011}\n",
      "Chapman, A. and Mesbahi, M. (2011).\n",
      "\\newblock Advection on graphs.\n",
      "\\newblock {\\em IEEE Conference on Decision and Control and European Control\n",
      "  Confereence (CDC-ECC)}, 50:1461--1466.\n",
      "\\bibitem[Chen et~al., 2001]{chen2001}\n",
      "Chen, C., Petty, K., Skabardonis, A., Varaiya, P., and Jia, Z. (2001).\n",
      "\\newblock Freeway performance measurement system: mining loop detector data.\n",
      "\\newblock {\\em Transportation Research Record}, 1748(1):96--102.\n",
      "\\bibitem[Chen et~al., 2018]{driving}\n",
      "Chen, Y., Sohani, N., and Peng, H. (2018).\n",
      "\\newblock Modelling of uncertain reactive human driving behavior: a\n",
      "  classification approach.\n",
      "\\newblock In {\\em 2018 IEEE Conference on Decision and Control (CDC)}, pages\n",
      "  3615--3621.\n",
      "\\bibitem[Gardner et~al., 2018]{gardner2018}\n",
      "Gardner, J., Pleiss, G., Bindel, D., Weinberger, K., and Wilson, A. (2018).\n",
      "\\newblock G{P}ytorch: Blackbox matrix-matrix gaussian process inference with\n",
      "  gpu acceleration.\n",
      "\\newblock {\\em 32nd Conference on Neural Information Processing Systems (NIPS\n",
      "  2018) arXiv:1809.11165v2}.\n",
      "\\bibitem[Gulian et~al., 2019]{osti_1642956}\n",
      "Gulian, M., Raissi, M., Perdikaris, P., and Karniadakis, G. (2019).\n",
      "\\newblock Machine learning of space-fractional differential equations, {SIAM}\n",
      "  {J}ournal on {S}cientific {C}omputing, {V}ol. 41, {N}o. 4, {S}ociety for\n",
      "  {I}ndustrial and {A}pplied {M}athematics.\n",
      "\\newblock pages A2485--A2509.\n",
      "\\bibitem[Hošek and Volek, 2019]{hosek2019}\n",
      "Hošek, R. and Volek, J. (2019).\n",
      "\\newblock Discrete advection–diffusion equations on graphs: Maximum principle\n",
      "  and finite volumes.\n",
      "\\newblock {\\em Applied Mathematics and Computation}, 361(C):630--644.\n",
      "\\bibitem[K\\\"{u}per and Waldherr, 2020]{kuper2020}\n",
      "K\\\"{u}per, A. and Waldherr, S. (2020).\n",
      "\\newblock Numerical gaussian process kalman filtering.\n",
      "\\newblock {\\em 21st IFAC World Congress}.\n",
      "\\bibitem[LeVeque, 2007]{leveque}\n",
      "LeVeque, R.~J. (2007).\n",
      "\\newblock {\\em Finite Difference Methods for Ordinary and Partial Differential\n",
      "  Equations: Steady-State and Time-Dependent Problems}.\n",
      "\\newblock SIAM.\n",
      "\\bibitem[Li et~al., 2018]{li2018}\n",
      "Li, Y., Yu, R., Shahabi, C., and Liu, Y. (2018).\n",
      "\\newblock Diffusion convolutional recurrent neural network: Data-driven traffic\n",
      "  forecasting.\n",
      "\\newblock {\\em International Conference on Learning Representations (ICLR)}.\n",
      "\\bibitem[Liberty et~al., 2020]{sagemaker}\n",
      "Liberty, E., Karnin, Z., Xiang, B., Rouesnel, L., Coskun, B., Nallapati, R.,\n",
      "  Delgado, J., Sadoughi, A., Astashonok, Y., Das, P., Balioglu, C.,\n",
      "  Chakravarty, S., Jha, M., Gautier, P., Arpin, D., Januschowski, T., Flunkert,\n",
      "  V., Wang, Y., Gasthaus, J., Stella, L., Rangapuram, S., Salinas, D.,\n",
      "  Schelter, S., and Smola, A. (2020).\n",
      "\\newblock Elastic machine learning algorithms in amazon sagemaker.\n",
      "\\newblock In {\\em 2020 ACM SIGMOD International Conference on Management of\n",
      "  Data, SIGMOD ’20, New York, NY, USA. Association for Computing Machinery.},\n",
      "  pages 731--737.\n",
      "\\bibitem[Liesen and Parlett, 2008]{liesen2008}\n",
      "Liesen, J. and Parlett, B.~N. (2008).\n",
      "\\newblock On nonsymmetric saddle point matrices that allow conjugate gradient\n",
      "  iterations.\n",
      "\\newblock {\\em Numer. Math.}, 108:605--624.\n",
      "\\bibitem[Lighthill and Whitham, 1955]{Lighthill1955OnKW}\n",
      "Lighthill, M. and Whitham, G. (1955).\n",
      "\\newblock On kinematic waves ii. a theory of traffic flow on long crowded\n",
      "  roads.\n",
      "\\newblock {\\em Proceedings of the Royal Society of London. Series A.\n",
      "  Mathematical and Physical Sciences}, 229:317 -- 345.\n",
      "\\bibitem[OpenStreetMap, 2017]{osm2017}\n",
      "OpenStreetMap (2017).\n",
      "\\newblock \\url {https://www.openstreetmap.org}.\n",
      "\\bibitem[Raissi et~al., 2019]{RAISSI2019686}\n",
      "Raissi, M., Perdikaris, P., and Karniadakis, G. (2019).\n",
      "\\newblock Physics-informed neural networks: A deep learning framework for\n",
      "  solving forward and inverse problems involving nonlinear partial differential\n",
      "  equations.\n",
      "\\newblock {\\em Journal of Computational Physics}, 378:686--707.\n",
      "\\bibitem[Rak, 2017]{thesis}\n",
      "Rak, A. (2017).\n",
      "\\newblock Advection on graphs.\n",
      "\\newblock \\url {http://nrs.harvard.edu/urn-3:HUL.InstRepos:38779537}.\n",
      "\\bibitem[Rasmussen and Williams, 2006]{rasmussen2006}\n",
      "Rasmussen, C. and Williams, C. (2006).\n",
      "\\newblock {\\em Gaussian Processes for Machine Learning}.\n",
      "\\newblock MIT Press.\n",
      "\\bibitem[Richards, 1956]{Richards1956}\n",
      "Richards, P. (1956).\n",
      "\\newblock Shock waves on the highway.\n",
      "\\newblock {\\em Operation Res.}, pages 42 -- 51.\n",
      "\\bibitem[Sen and Venkaiah, 1988]{sen1988}\n",
      "Sen, S. and Venkaiah, V.~C. (1988).\n",
      "\\newblock On symmetrizing a matrix.\n",
      "\\newblock {\\em Indian J. pure appl. Math.}, 19(6):554--561.\n",
      "\\bibitem[Solomon, 2015]{solomon2015pde}\n",
      "Solomon, J. (2015).\n",
      "\\newblock {PDE} approaches to graph analysis.\n",
      "\\newblock {\\em ArXiv}, abs/1505.00185.\n",
      "\\end{thebibliography}\n",
      "{apalike}\n",
      "\\appendix\n",
      "   \n",
      "  \n",
      "\\newpage\n",
      "\\newpage\n",
      "\\section{Upwinding discretizations of linear advection}\n",
      "  We discretize the 1D linear advection equation with velocity $v$:\n",
      "    \\begin{equation*}\n",
      "        u_t + vu_x = 0,\n",
      "    \\end{equation*}\n",
      "using the standard first order upwinding scheme on a simple uniform Cartesian mesh with spatial step size $\\Delta x$.  \n",
      "Then the classical finite difference first-order upwind scheme depends on the sign of $v$.  For flow moving from left to right, $v > 0$, and we have the following semi-discrete discretization \\cite{leveque}:\n",
      "    \\begin{equation}\n",
      "    \\begin{aligned}\n",
      "        & \\frac{du_i}{dt} + v \\frac{u_i - u_{i-1}}{\\Delta x} = 0, \\hspace{0.25cm} \\text{if} \\hspace{0.25cm} v > 0, \\\\\n",
      "         & \\frac{du_i}{dt} + v \\frac{u_{i+1} - u_{i}}{\\Delta x} = 0, \\hspace{0.25cm} \\text{if} \\hspace{0.25cm} v < 0. \\\\\n",
      "    \\end{aligned}\n",
      "    \\label{eqn:first_order_upwind}\n",
      "    \\end{equation}\n",
      "    Upwinding schemes are useful in the advection case since information is moving from left to right.  The Courant-Friedrichs-Lewy (CFL) condition for stability of the first order upwinding scheme with Forward Euler time-stepping discretization with time step $\\Delta t$ is given by:\n",
      "$$ \\Bigl \\lvert \\frac{v \\Delta t}{ \\Delta x}\\Bigr \\rvert \\le 1 \\iff \\Delta t \\le \\Bigl \\lvert \\frac{v}{ \\Delta x}\\Bigr \\rvert.$$\n",
      "    \\label{appendix:upwind}\n",
      "A less diffusive second order upwind scheme is also known as linear upwind differencing (LUD), and is given by:\n",
      "\\begin{equation}\n",
      "\\frac{du_i}{dt} = v \\frac{-u_{i-2} + 4u_{i-1}-3u_i}{2 \\Delta x}.\n",
      "\\label{eqn:LUD}\n",
      "\\end{equation}\n",
      "We can show that the scheme is second-order accurate using Taylor expansions. It is designed to be less diffusive because the $u_{xx}$ term from the first-order upwinding scheme cancels.\n",
      "We have \n",
      "\\begin{align*}\n",
      "    \\frac{u_{i-2} - 4u_{i-1}+3u_i}{2 \\Delta x} &= \\frac{1}{2\\Delta x} \\Bigg[ \\Big( u-2 \\Delta x u_x + \\frac{4\\Delta x^2}{2} u_{xx} - \\frac {8\\Delta x^3}{6} u_{xxx} + \\mathcal{O}(\\Delta x ^4) \\Big) \\\\\n",
      "    & + \\Big( - 4(u-\\Delta x u_x + \\frac{\\Delta x^2}{2} u_{xx} - \\frac {\\Delta x^3}{6} u_{xxx} + \\mathcal{O}(\\Delta x ^4)) \\Big)\n",
      " + 3u \\Bigg] \\\\\n",
      "    &= u_x -  \\frac {\\Delta x^2}{3} u_{xxx} + \\mathcal{O}(\\Delta x ^4).\n",
      "\\end{align*}\n",
      "Hence, the scheme is second order accurate with a dispersive $u_{xxx}$ leading error term.\n",
      "\\section{Examples of $L_{adv}$ on balanced graphs resulting in finite difference discretizations of linear advection}\n",
      "\\label{app:finite_diff_graphs}\n",
      "In addition to the finite difference schemes provided in Section \\ref{subsec:finite_diff}, we also provide an example of a non-uniform mesh discretization:\n",
      "$$ \\frac{du_i}{dx} \\approx \\frac{\\frac{4}{3} u_{i+1/2} - u_i - \\frac{1}{3} u_{i-1}}{\\Delta x}, $$\n",
      "which results in the following graph, where the in-going and out-going edges from $u_i$:\n",
      "\\begin{center}\n",
      "\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,\n",
      "                    thick,main node/.style={circle,draw,font=\\sffamily\\bfseries}]\n",
      "                    \\centering\n",
      "  \\node[main node] (1) {$u_{i-1}$};\n",
      "  \\node[main node] (2)[right of=1] {$u_{i-1/2}$};\n",
      "  \\node[main node] (3) [right of=2] {$u_{i}$};\n",
      "  \\node[main node] (4) [right of=3] {$u_{i+1/2}$};\n",
      "  \\node[main node] (5) [right of=4] {$u_{i+1}$};\n",
      "  \\path[every node/.style={font=\\sffamily\\small}]\n",
      "    (1) edge [bend right] node[below] {$v/3\\Delta{x}$} (3)\n",
      "    (2) edge [bend right] node[below] {$v/3\\Delta{x}$} (4)\n",
      "    (2) edge node [above] {$-4v/3\\Delta{x}$} (1) \n",
      "    (4) edge node [above] {$-4v/3\\Delta{x}$} (3) \n",
      "    (3) edge node [above] {$-4v/3\\Delta{x}$} (2) \n",
      "    (5) edge node [above] {$-4v/3\\Delta{x}$} (4) \n",
      "    (3) edge [bend right] node[below] {$v/3\\Delta{x}$} (5);\n",
      "    \n",
      "    \n",
      "\\end{tikzpicture}\n",
      "\\end{center}\n",
      "\\vspace{-.25cm}\n",
      "We can obtain the less diffusive second order upwind scheme (LUD) in \\eqref{eqn:LUD} using the following graph: \n",
      "\\begin{center}\n",
      "\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,\n",
      "                    thick,main node/.style={circle,draw,font=\\sffamily\\bfseries}]\n",
      "                    \\centering\n",
      "  \\node[main node] (1) {$u_{i-2}$};\n",
      "  \\node[main node] (2) [right of=1] {$u_{i-1}$};\n",
      "  \\node[main node] (3) [right of=2] {$u_{i}$};\n",
      "\\node[main node] (4) [right of=3] {$u_{i+1}$};\n",
      "  \\node[main node] (5) [right of=4] {$u_{i+2}$};\n",
      "  \\path[every node/.style={font=\\sffamily\\small}]\n",
      "  (1) edge [bend right] node[below] {$-v/2\\Delta{x}$} (3)\n",
      "    (1) edge node[above] {$2v/\\Delta{x}$} (2)\n",
      "    (2) edge node [above] {$2v/\\Delta{x}$} (3) \n",
      "     (2) edge [bend right] node[below] {$-v/2\\Delta{x}$} (4)\n",
      "    (3) edge node[above] {$2v/\\Delta{x}$} (4)\n",
      "     (3) edge [bend right] node[below] {$-v/2\\Delta{x}$} (5)\n",
      "    (4) edge node [above] {$2v/\\Delta{x}$} (5) ;\n",
      "\\end{tikzpicture}\n",
      "\\end{center}\n",
      "\\section{Additional Experiments}\n",
      "\\label{experiments}\n",
      "       \\subsection{Gaussian Process prior results with DGAMGP}\n",
      "       \\label{prior}\n",
      "       \n",
      "        A main property of the Mat\\'ern Gaussian Process kernel is that it varies along Riemannian manifolds.   The variance of the kernel is a function of degree, and depends on a complex manner on the graph.\n",
      "         We show the results generated with a star graph directed towards the center node and a directed complete graph. \n",
      "         Figure \\ref{subfig:complete} shows that as expected for the complete graph, the nodes have the same variability, since for a random walk starting from any node, there is equal probability to get to another node. For the star graph in Figure \\ref{subfig:star}, we observe that the center node has a variability of approximately 0 as starting from any node on the graph, the random walk always ends at the center.\n",
      "      \n",
      "        \n",
      "\\begin{figure}[h!]\n",
      "\\centering     \n",
      "\\subfigure[complete graph prior.]{\\label{subfig:complete}[width=0.4\\textwidth]{complete-prior.png}}\n",
      "\\subfigure[star graph prior.]{\\label{subfig:star}[width=0.4\\textwidth]{star-prior.png}}\n",
      "\\caption{Prior results using DGAMGP obtained using various graphs, and plotting tools from \\cite{borovitskiy2021matern}.} \n",
      "\\label{Velocities}\n",
      "\\end{figure}\n",
      "         \n",
      "\\subsection{Convergence Studies}\n",
      "We conduct a convergence study of applying  $L_{adv}$ on the upwind graph in Figure \\ref{fig:graph_upwind}, and show that it has first order convergence matching the performance of the equivalent first order upwind scheme. We use the same initial condition as in Figure \\ref{fig:upwind_exact}. We then solve the resulting system of ODEs using the RK5 ODE solver. Figure \\ref{fig:upwind} shows the solution at different time steps, and we see how the solution is propagating to the right.  Figure \\ref{Conv_anal} shows a loglog plot, where the error is decreasing linearly with a slope of 1 as the number of nodes $n$ is increasing, as expected. \n",
      "\\begin{figure}[H]\n",
      "\\centering     \n",
      "\\subfigure[Solution of the linear advection equation using \\eqref{eqn:graph_adv} ]{\\label{fig:upwind}[width=0.49\\textwidth]{solution_upwind.png}}\n",
      "\\subfigure[Convergence study in a log-log plot]{\\label{Conv_anal}[width=0.49\\textwidth]{Conv_analysis.png}}\n",
      "\\caption{Upwinding solution with RK5 to the linear advection equation over time and corresponding convergence study.}\n",
      "\\label{solve_upwind_conv}\n",
      "\\end{figure}\n",
      "\\end{document}\n",
      "iteration:: 0 ('/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00001.pdf', '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/latex/2201.00001.tex')\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    print(\"iteration::\",i, sample)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding Details\n",
    "import json\n",
    "json_path = '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'\n",
    "with open(json_path, \"r\") as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(loaded_data[0])):\n",
    "    count =0\n",
    "    for i in range(len(loaded_data[0][0])):\n",
    "        if loaded_data[0][j][i]!=0 and loaded_data[0][0][i]!=1:\n",
    "            # print(loaded_data[0][0][i])\n",
    "            count+=1\n",
    "    print(\"count: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"class pdfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                dataset_path,\n",
    "                split: str = \"traisn\",\n",
    "                root_name: str = \"pdf\",):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.root_name = root_name\n",
    "        self.path_to_root = Path(dataset_path).parent\n",
    "        self.root_name = root_name\n",
    "        # self.path_to_root = self.path_to_root.parent\n",
    "\n",
    "        # if not split in self.dataset_path:\n",
    "        #     pti = self.path_to_root / (template % split + \".jsonl\")\n",
    "        #     if pti.exists():\n",
    "        #         self.dataset_path = pti\n",
    "        #     else:\n",
    "        #         raise ValueError(f'Dataset file for split \"{split}\" not found: {pti}')\n",
    "        \n",
    "        self.dataset_file = None  # mulitprocessing\n",
    "        \n",
    "        if self.dataset_file is None:\n",
    "            self.dataset_file = Path(self.dataset_path).open()\n",
    "        line = self.dataset_file.readline()\n",
    "        data: Dict = orjson.loads(line)\n",
    "\n",
    "        pdf_path: Path = self.path_to_root / self.root_name / data.pop(\"pdf\")\n",
    "        latex_path: Path = self.path_to_root / self.root_name / data.pop(\"latex\")\n",
    "\n",
    "        self.dataset = {\"pdf\": pdf_path, \"ground_truth\": latex_path}\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.dataset[idx]\n",
    "        # print(\"idx:\", idx)\n",
    "        # print(\"sample:\", sample)\n",
    "        return sample\n",
    "        # if sample is None:\n",
    "        #     # if sample is broken choose another randomly\n",
    "        #     return self[random.randint(0, self.dataset_length - 1)]\n",
    "        # if sample is None or sample[\"image\"] is None or prod(sample[\"image\"].size) == 0:\n",
    "        #     input_tensor = None\n",
    "        # else:\n",
    "        #     input_tensor = self.nougat_model.encoder.prepare_input(\n",
    "        #         sample[\"image\"], random_padding=self.split == \"train\"\n",
    "                \n",
    "        #     )\n",
    "\n",
    "        # tokenizer_out = self.nougat_model.decoder.tokenizer(\n",
    "        #     sample[\"ground_truth\"],\n",
    "        #     max_length=self.max_length,\n",
    "        #     padding=\"max_length\",\n",
    "        #     return_token_type_ids=False,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # input_ids = tokenizer_out[\"input_ids\"].squeeze(0)\n",
    "        # attention_mask = tokenizer_out[\"attention_mask\"].squeeze(0)\n",
    "        # # randomly perturb ground truth tokens\n",
    "        # if self.split == \"train\" and self.perturb:\n",
    "        #     # check if we perturb tokens\n",
    "        #     unpadded_length = attention_mask.sum()\n",
    "        #     while random.random() < 0.1:\n",
    "        #         try:\n",
    "        #             pos = random.randint(1, unpadded_length - 2)\n",
    "        #             token = random.randint(\n",
    "        #                 23, len(self.nougat_model.decoder.tokenizer) - 1\n",
    "        #             )\n",
    "        #             input_ids[pos] = token\n",
    "        #         except ValueError:\n",
    "        #             break\n",
    "        # return input_tensor, input_ids, attention_mask\n",
    "    def __iter__(self):\n",
    "        for i in range(self.dataset_length):\n",
    "            yield self[i]\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
